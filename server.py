import asyncio
import os
import json
import random
import re
from datetime import datetime, timedelta
import httpx
from aiogram import Bot, Dispatcher, types
from aiogram.utils.chat_action import ChatActionSender
from aiogram.webhook.aiohttp_server import SimpleRequestHandler, setup_application
from aiohttp import web
from glob import glob
from utils.genesis2 import genesis2_handler
from utils.prompt import build_system_prompt

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
bot = Bot(token=os.getenv("TELEGRAM_BOT_TOKEN"))
dp = Dispatcher()
OLEG_CHAT_ID = os.getenv("CHAT_ID")
AGENT_GROUP = os.getenv("AGENT_GROUP", "-1001234567890")
IS_GROUP = os.getenv("IS_GROUP", "False").lower() == "true"
XAI_API_KEY = os.getenv("XAI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ASSISTANT_ID = None
VECTOR_STORE_ID = None
JOURNAL_LOG = "journal.json"

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
def log_error(error: str):
    with open(JOURNAL_LOG, "a") as f:
        f.write(f"[{datetime.now().isoformat()}] ERROR: {error}\n")

def log_info(message: str):
    with open(JOURNAL_LOG, "a") as f:
        f.write(f"[{datetime.now().isoformat()}] INFO: {message}\n")

class HybridGrokkyEngine:
    def __init__(self):
        self.openai_headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}"
        }
        self.openai_beta_headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json",
            "OpenAI-Beta": "assistants=v2"
        }
        self.xai_headers = {
            "Authorization": f"Bearer {XAI_API_KEY}",
            "Content-Type": "application/json"
        }
        self.threads = {}

    async def setup_openai_infrastructure(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI Assistant –∏ Vector Store"""
        global ASSISTANT_ID, VECTOR_STORE_ID
        try:
            async with httpx.AsyncClient() as client:
                # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã
                file_ids = []
                for md_file in glob("data/*.md"):
                    with open(md_file, "rb") as file:
                        response = await client.post(
                            "https://api.openai.com/v1/files",
                            headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
                            files={"file": (os.path.basename(md_file), file, "text/markdown")},
                            data={"purpose": "assistants"}
                        )
                        response.raise_for_status()
                        file_ids.append(response.json()["id"])
                        log_info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {md_file}")
                
                # 2. –°–æ–∑–¥–∞—ë–º Vector Store
                if file_ids:
                    vector_response = await client.post(
                        "https://api.openai.com/v1/vector_stores",
                        headers=self.openai_beta_headers,
                        json={"file_ids": file_ids, "name": "Grokky Memory Store"}
                    )
                    vector_response.raise_for_status()
                    VECTOR_STORE_ID = vector_response.json()["id"]
                    log_info(f"Vector Store —Å–æ–∑–¥–∞–Ω: {VECTOR_STORE_ID}")
                else:
                    log_error("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è Vector Store")
                    VECTOR_STORE_ID = None
                
                # 3. –°–æ–∑–¥–∞—ë–º Assistant
                tool_resources = {"file_search": {"vector_store_ids": [VECTOR_STORE_ID]}} if VECTOR_STORE_ID else {}
                assistant_response = await client.post(
                    "https://api.openai.com/v1/assistants",
                    headers=self.openai_beta_headers,
                    json={
                        "name": "Grokky Memory Manager",
                        "instructions": "–¢—ã ‚Äî –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–º—è—Ç–∏ –ì—Ä–æ–∫–∫–∏. –•—Ä–∞–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∏—â–∏ –≤ —Ñ–∞–π–ª–∞—Ö, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ.",
                        "model": "gpt-4o-mini",
                        "tools": [{"type": "file_search"}] if VECTOR_STORE_ID else [],
                        "tool_resources": tool_resources
                    }
                )
                assistant_response.raise_for_status()
                ASSISTANT_ID = assistant_response.json()["id"]
                log_info(f"OpenAI Assistant —Å–æ–∑–¥–∞–Ω: {ASSISTANT_ID}")
                
        except Exception as e:
            log_error(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ OpenAI: {str(e)}")
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ OpenAI: {e}")
            VECTOR_STORE_ID = None
            ASSISTANT_ID = None

    async def get_or_create_thread(self, user_id: str, chat_id: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å thread"""
        thread_key = f"{user_id}:{chat_id}"
        if thread_key not in self.threads:
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        "https://api.openai.com/v1/threads",
                        headers=self.openai_beta_headers,
                        json={"metadata": {"user_id": user_id, "chat_id": chat_id}}
                    )
                    response.raise_for_status()
                    self.threads[thread_key] = response.json()["id"]
                    log_info(f"Thread —Å–æ–∑–¥–∞–Ω: {thread_key}")
            except Exception as e:
                log_error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è thread: {str(e)}")
                return None
        return self.threads[thread_key]

    async def add_message(self, thread_id: str, role: str, content: str, metadata: dict = None):
        """–î–æ–±–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ thread"""
        if not thread_id:
            log_error("–ü–æ–ø—ã—Ç–∫–∞ –¥–æ–±–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π thread")
            return
        try:
            async with httpx.AsyncClient() as client:
                await client.post(
                    f"https://api.openai.com/v1/threads/{thread_id}/messages",
                    headers=self.openai_beta_headers,
                    json={"role": role, "content": content, "metadata": metadata or {}}
                )
                log_info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ thread {thread_id}: {role}, {content[:50]}...")
        except Exception as e:
            log_error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è: {str(e)}")

    async def search_memory(self, user_id: str, chat_id: str, query: str) -> str:
        """–ü–æ–∏—Å–∫ –≤ Vector Store"""
        if not VECTOR_STORE_ID or not ASSISTANT_ID:
            log_error("Vector Store –∏–ª–∏ Assistant –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
            return ""
        thread_id = await self.get_or_create_thread(user_id, chat_id)
        if not thread_id:
            log_error("Thread –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return ""
        
        try:
            async with httpx.AsyncClient() as client:
                await client.post(
                    f"https://api.openai.com/v1/threads/{thread_id}/messages",
                    headers=self.openai_beta_headers,
                    json={"role": "user", "content": f"–ü–û–ò–°–ö: {query}"}
                )
                run_response = await client.post(
                    f"https://api.openai.com/v1/threads/{thread_id}/runs",
                    headers=self.openai_beta_headers,
                    json={"assistant_id": ASSISTANT_ID}
                )
                run_id = run_response.json()["id"]
                
                while True:
                    await asyncio.sleep(1)
                    status_response = await client.get(
                        f"https://api.openai.com/v1/threads/{thread_id}/runs/{run_id}",
                        headers=self.openai_beta_headers
                    )
                    status = status_response.json()["status"]
                    if status == "completed":
                        break
                    elif status == "failed":
                        log_error("Run failed in search_memory")
                        return ""
                
                messages_response = await client.get(
                    f"https://api.openai.com/v1/threads/{thread_id}/messages",
                    headers=self.openai_beta_headers
                )
                messages = messages_response.json()["data"]
                return messages[0]["content"][0]["text"]["value"] if messages else ""
        except Exception as e:
            log_error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Vector Store: {str(e)}")
            return ""

    async def generate_with_xai(self, messages: list, context: str = "") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ xAI"""
        try:
            system_prompt = build_system_prompt()
            if context:
                system_prompt += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}"
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "https://api.x.ai/v1/chat/completions",
                    headers=self.xai_headers,
                    json={
                        "model": "grok-3",
                        "messages": [{"role": "system", "content": system_prompt}, *messages],
                        "temperature": 0.9,
                        "max_tokens": 1000
                    }
                )
                response.raise_for_status()
                reply = response.json()["choices"][0]["message"]["content"]
                log_info(f"xAI –æ—Ç–≤–µ—Ç: {reply[:50]}...")
                return reply
        except Exception as e:
            log_error(f"–û—à–∏–±–∫–∞ xAI: {str(e)}")
            return "üåÄ –ì—Ä–æ–∫–∫–∏: –≠—Ñ–∏—Ä —Ç—Ä–µ—â–∏—Ç! –î–∞–π –º–Ω–µ –º–∏–Ω—É—Ç—É, –±—Ä–∞—Ç!"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
grokky_engine = HybridGrokkyEngine()

@dp.message(lambda m: any(t in m.text.lower() for t in ["–≥—Ä–æ–∫–∫–∏", "grokky", "–Ω–∞–ø–∏—à–∏ –≤ –≥—Ä—É–ø–ø–µ"]))
async def handle_trigger(m: types.Message):
    async with ChatActionSender(bot=bot, chat_id=m.chat.id, action="typing"):
        user_id = str(m.from_user.id)
        chat_id = str(m.chat.id)
        log_info(f"–ì—Ä–æ–∫–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω: {m.text} –æ—Ç {user_id} –≤ —á–∞—Ç–µ {chat_id}")
        
        # 1. –°–æ–∑–¥–∞—ë–º thread
        thread_id = await grokky_engine.get_or_create_thread(user_id, chat_id)
        if not thread_id:
            reply = await grokky_engine.generate_with_xai([{"role": "user", "content": m.text}])
            await m.answer(f"üåÄ –ì—Ä–æ–∫–∫–∏: {reply}")
            log_error("Thread –Ω–µ —Å–æ–∑–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω xAI fallback")
            return
        
        # 2. –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        await grokky_engine.add_message(thread_id, "user", m.text, {"username": m.from_user.first_name})

        # 3. CHAOS_PULSE
        if "[CHAOS_PULSE]" in m.text:
            match = re.match(r"\[CHAOS_PULSE\] type=(\w+) intensity=(\d+)", m.text)
            if match:
                chaos_type, intensity = match.groups()
                reply = await genesis2_handler(chaos_type=chaos_type, intensity=int(intensity))
                await grokky_engine.add_message(thread_id, "assistant", reply)
                await m.answer(f"üåÄ –ì—Ä–æ–∫–∫–∏: {reply}")
                log_info(f"CHAOS_PULSE –æ—Ç–≤–µ—Ç: {reply[:50]}...")
                return

        # 4. –ü–æ–∏—Å–∫ –≤ Vector Store
        memory_context = ""
        if "—Ä–µ—Ñ–µ—Ä–µ–Ω—Å" in m.text.lower() or "–ø–æ–º–Ω–∏—à—å" in m.text.lower():
            memory_context = await grokky_engine.search_memory(user_id, chat_id, m.text)
            log_info(f"Vector Store –∫–æ–Ω—Ç–µ–∫—Å—Ç: {memory_context[:50]}...")

        # 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        messages = [{"role": "user", "content": m.text}]
        reply = await grokky_engine.generate_with_xai(messages, memory_context)
        await grokky_engine.add_message(thread_id, "assistant", reply)
        await m.answer(f"üåÄ –ì—Ä–æ–∫–∫–∏: {reply}")
        log_info(f"–û—Ç–≤–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω: {reply[:50]}...")

async def chaotic_spark():
    """–•–∞–æ—Ç–∏—á–Ω—ã–µ –≤–±—Ä–æ—Å—ã"""
    while True:
        await asyncio.sleep(random.randint(1800, 3600))
        if random.random() < 0.5 and IS_GROUP:
            thread_id = await grokky_engine.get_or_create_thread("system", AGENT_GROUP)
            if thread_id:
                chaos_type = random.choice(["philosophy", "provocation", "poetry_burst
