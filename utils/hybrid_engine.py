import asyncio
import os
import json
import re
import random
from datetime import datetime
import httpx
from openai import OpenAI

class HybridGrokkyEngine:
    def __init__(self):
        # Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸
        self.openai_h = {"Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}", "Content-Type": "application/json"}
        self.xai_h = {"Authorization": f"Bearer {os.getenv('XAI_API_KEY')}", "Content-Type": "application/json"}
        self.memory_path = "data/memory/"
        os.makedirs(self.memory_path, exist_ok=True)
        
        # ÐÐ¾Ð²Ñ‹Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð´Ð»Ñ Assistants API
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.assistant_id = os.getenv("ASSISTANT_ID")
        self.threads = {}  # chat_id -> thread_id
        self._load_thread_mapping()

    async def setup_openai_infrastructure(self):
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ð°Ð¼ÑÑ‚Ð¸

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð° ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        if not self.assistant_id:
            from utils.prompt import build_system_prompt
            system_prompt = build_system_prompt()
            assistant = self.openai_client.beta.assistants.create(
                name="Grokky",
                instructions=system_prompt,
                model="gpt-4o",
                tools=[{"type": "code_interpreter"}]
            )
            self.assistant_id = assistant.id
            print(f"Ð¡Ð¾Ð·Ð´Ð°Ð½ Ð½Ð¾Ð²Ñ‹Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ñ ID: {self.assistant_id}")
        return True

    async def add_memory(self, user_id, text, role="user"):
        """Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ"""
        filename = f"{self.memory_path}/{user_id}.jsonl"
        try:
            memory_item = {
                "timestamp": datetime.now().isoformat(),
                "role": role,
                "content": text
            }
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² Ñ„Ð°Ð¹Ð»Ð¾Ð²ÑƒÑŽ Ð¿Ð°Ð¼ÑÑ‚ÑŒ
            with open(filename, "a+", encoding="utf-8") as f:
                f.write(f"{json.dumps(memory_item, ensure_ascii=False)}\n")
                
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð² Ñ‚Ñ€ÐµÐ´ Assistants API, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
            if user_id in self.threads and self.assistant_id:
                try:
                    self.openai_client.beta.threads.messages.create(
                        thread_id=self.threads[user_id],
                        role=role,
                        content=text
                    )
                except Exception as e:
                    print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð² Ñ‚Ñ€ÐµÐ´ Assistant: {e}")
            
            return True
        except Exception as e:
            print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ: {e}")
            return False

    async def search_memory(self, user_id, query, limit=5):
        """Ð˜Ñ‰ÐµÑ‚ Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ"""
        filename = f"{self.memory_path}/{user_id}.jsonl"
        if not os.path.exists(filename):
            return ""
        
        # Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ ÐºÐ¾Ð´ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸...
        
        # Ð•ÑÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Assistant API, Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð¸ÑÐº ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ñ‚Ð°Ð¼
        if user_id in self.threads and self.assistant_id:
            try:
                # ÐŸÐ¾ÐºÐ° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½ÑƒÑŽ Ñ„Ð°Ð¹Ð»Ð¾Ð²ÑƒÑŽ Ð¿Ð°Ð¼ÑÑ‚ÑŒ
                pass
            except Exception as e:
                print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ° Ð² Ñ‚Ñ€ÐµÐ´Ðµ Assistant: {e}")
        
        return ""  # Ð¸Ð»Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¿Ð¾Ð¸ÑÐºÐ°

    async def generate_with_xai(self, messages, context=""):
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚ Ñ‡ÐµÑ€ÐµÐ· xAI Grok"""
        from utils.prompt import build_system_prompt
        
        # Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ ÐºÐ¾Ð´...
        system_prompt = build_system_prompt()
        
        if context:
            system_prompt = f"{system_prompt}\n\nContext for answering: {context}"
            
        full_messages = [{"role": "system", "content": system_prompt}] + messages
            
        try:
            async with httpx.AsyncClient() as client:
                res = await client.post(
                    "https://api.x.ai/v1/chat/completions",
                    headers=self.xai_h,
                    json={"model": "grok-3", "messages": full_messages, "temperature": 1.0}
                )
                res.raise_for_status()
                return res.json()["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"ÐžÑˆÐ¸Ð±ÐºÐ° xAI: {e}")
            return "ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ²ÑÐ·Ð¸ Ñ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÑŒÑŽ, ÑÑ„Ð¸Ñ€ Ñ‚Ñ€ÐµÑ‰Ð¸Ñ‚!"

    async def generate_with_assistant(self, chat_id, messages=None):
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Assistants API"""
        if not self.assistant_id:
            await self.setup_openai_infrastructure()
            
        thread_id = await self.get_or_create_thread(chat_id)
        
        # Ð•ÑÐ»Ð¸ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ñ‹ Ð½Ð¾Ð²Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ, Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ñ…
        if messages:
            for msg in messages:
                self.openai_client.beta.threads.messages.create(
                    thread_id=thread_id,
                    role=msg["role"],
                    content=msg["content"]
                )
        
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ
        run = self.openai_client.beta.threads.runs.create(
            thread_id=thread_id,
            assistant_id=self.assistant_id
        )
        
        # Ð–Ð´ÐµÐ¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
        run = self._wait_for_run_completion(thread_id, run.id)
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
        messages = self.openai_client.beta.threads.messages.list(
            thread_id=thread_id,
            order="desc",
            limit=1
        )
        
        if not messages.data:
            return "ðŸŒ€ Ð“Ñ€Ð¾ÐºÐºÐ¸ Ð½Ðµ ÑÐ¼Ð¾Ð³ ÑÑ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚"
        
        return messages.data[0].content[0].text.value
    
    async def get_or_create_thread(self, chat_id):
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ Ð¸Ð»Ð¸ ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ´ Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð°"""
        if chat_id not in self.threads:
            thread = self.openai_client.beta.threads.create()
            self.threads[chat_id] = thread.id
            self._save_thread_mapping()
            return thread.id
        return self.threads[chat_id]
    
    def _wait_for_run_completion(self, thread_id, run_id):
        """Ð–Ð´ÐµÑ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Run"""
        import time
        while True:
            run = self.openai_client.beta.threads.runs.retrieve(
                thread_id=thread_id,
                run_id=run_id
            )
            if run.status == 'completed':
                return run
            elif run.status in ['failed', 'cancelled', 'expired']:
                print(f"Run failed with status: {run.status}")
                return run
            time.sleep(1)
    
    def _save_thread_mapping(self):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ chat_id -> thread_id Ð² Ñ„Ð°Ð¹Ð»"""
        with open("data/thread_mapping.json", "w") as f:
            json.dump(self.threads, f)
    
    def _load_thread_mapping(self):
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ chat_id -> thread_id Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð°"""
        try:
            with open("data/thread_mapping.json", "r") as f:
                self.threads = json.load(f)
        except FileNotFoundError:
            self.threads = {}
