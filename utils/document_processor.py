"""
Grokky AI Assistant - Document Processor
–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å—Å—ã–ª–æ–∫ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""

import os
import re
import asyncio
import random
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import aiohttp
import PyPDF2
from docx import Document
from newspaper import Article
from readability import Document as ReadabilityDocument
from bs4 import BeautifulSoup
from utils.journal import log_event, wilderness_log
from utils.telegram_utils import get_file_url, send_telegram_message_async

class DocumentProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—Å—ã–ª–æ–∫"""
    
    def __init__(self, memory_engine):
        self.memory_engine = memory_engine
        self.max_text_size = int(os.getenv("MAX_TEXT_SIZE", "3500"))
        
    async def process_url(self, url: str, user_id: str, chat_id: str, author_name: str) -> Dict:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç URL –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            content = await self._extract_url_content(url)
            
            if not content or content.startswith("[–û—à–∏–±–∫–∞"):
                return {
                    "success": False,
                    "message": f"{author_name}, –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Å—ã–ª–∫—É: {content}",
                    "content": None
                }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
            context_type = "group" if chat_id == os.getenv("AGENT_GROUP") else "personal"
            memory_result = await self.memory_engine.add_memory(
                user_id=user_id,
                message=f"[URL] {url}: {content}",
                chat_id=chat_id,
                context_type=context_type,
                author_name=author_name
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ
            summary = self._generate_summary(content, url)
            
            # –õ–æ–≥–∏—Ä—É–µ–º
            log_event({
                "type": "url_processed",
                "url": url,
                "author": author_name,
                "chat_id": chat_id,
                "user_id": user_id,
                "content_length": len(content),
                "memory_saved": memory_result is not None
            })
            
            return {
                "success": True,
                "message": f"{author_name}, –æ–±—Ä–∞–±–æ—Ç–∞–ª —Å—Å—ã–ª–∫—É! {summary}",
                "content": content,
                "summary": summary
            }
            
        except Exception as e:
            error_msg = f"–ì—Ä–æ–∫–∫–∏ –≤–∑–æ—Ä–≤–∞–ª—Å—è –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Å—ã–ª–∫–∏: {e}"
            return {
                "success": False,
                "message": f"{author_name}, {error_msg}",
                "content": None
            }
    
    async def process_document(self, file_id: str, file_name: str, user_id: str, chat_id: str, author_name: str) -> Dict:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º URL —Ñ–∞–π–ª–∞
            file_url = await get_file_url(file_id)
            if not file_url:
                return {
                    "success": False,
                    "message": f"{author_name}, –Ω–µ —Å–º–æ–≥ –ø–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª!",
                    "content": None
                }
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            file_ext = file_name.lower().split('.')[-1] if '.' in file_name else ''
            
            if file_ext == 'pdf':
                content = await self._extract_pdf_content(file_url)
            elif file_ext in ['doc', 'docx']:
                content = await self._extract_docx_content(file_url)
            elif file_ext in ['txt', 'md']:
                content = await self._extract_text_content(file_url)
            else:
                return {
                    "success": False,
                    "message": f"{author_name}, –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é —Ñ–æ—Ä–º–∞—Ç .{file_ext}! –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é: PDF, DOC, DOCX, TXT, MD",
                    "content": None
                }
            
            if not content or content.startswith("[–û—à–∏–±–∫–∞"):
                return {
                    "success": False,
                    "message": f"{author_name}, –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {content}",
                    "content": None
                }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
            context_type = "group" if chat_id == os.getenv("AGENT_GROUP") else "personal"
            memory_result = await self.memory_engine.add_memory(
                user_id=user_id,
                message=f"[DOCUMENT] {file_name}: {content}",
                chat_id=chat_id,
                context_type=context_type,
                author_name=author_name
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ
            summary = self._generate_summary(content, file_name)
            
            # –õ–æ–≥–∏—Ä—É–µ–º
            log_event({
                "type": "document_processed",
                "file_name": file_name,
                "file_type": file_ext,
                "author": author_name,
                "chat_id": chat_id,
                "user_id": user_id,
                "content_length": len(content),
                "memory_saved": memory_result is not None
            })
            
            return {
                "success": True,
                "message": f"{author_name}, –æ–±—Ä–∞–±–æ—Ç–∞–ª –¥–æ–∫—É–º–µ–Ω—Ç '{file_name}'! {summary}",
                "content": content,
                "summary": summary
            }
            
        except Exception as e:
            error_msg = f"–ì—Ä–æ–∫–∫–∏ –≤–∑–æ—Ä–≤–∞–ª—Å—è –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}"
            return {
                "success": False,
                "message": f"{author_name}, {error_msg}",
                "content": None
            }
    
    async def _extract_url_content(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ URL"""
        try:
            # –ü—Ä–æ–±—É–µ–º newspaper3k –¥–ª—è —Å—Ç–∞—Ç–µ–π
            try:
                article = Article(url, language='ru')
                article.download()
                article.parse()
                
                if article.text and len(article.text) > 100:
                    title = article.title or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                    content = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n\n{article.text}"
                    return content[:self.max_text_size]
            except:
                pass
            
            # Fallback –Ω–∞ readability
            try:
                headers = {
                    "User-Agent": "Mozilla/5.0 (Grokky Agent) AppleWebKit/537.36"
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=15, headers=headers) as response:
                        response.raise_for_status()
                        html = await response.text()
                
                doc = ReadabilityDocument(html)
                title = doc.title() or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                content = BeautifulSoup(doc.summary(), 'html.parser').get_text()
                
                result = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n\n{content}"
                return result[:self.max_text_size]
                
            except Exception as e:
                return f"[–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}]"
                
        except Exception as e:
            return f"[–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL: {e}]"
    
    async def _extract_pdf_content(self, file_url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(file_url) as response:
                    response.raise_for_status()
                    pdf_data = await response.read()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ
            temp_path = f"/tmp/grokky_temp_{random.randint(1000, 9999)}.pdf"
            with open(temp_path, 'wb') as f:
                f.write(pdf_data)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text_content = []
            with open(temp_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text_content.append(page.extract_text())
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.remove(temp_path)
            
            content = "\n".join(text_content)
            return content[:self.max_text_size] if content else "[PDF –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —á–∏—Ç–∞–µ—Ç—Å—è]"
            
        except Exception as e:
            return f"[–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF: {e}]"
    
    async def _extract_docx_content(self, file_url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ DOCX"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(file_url) as response:
                    response.raise_for_status()
                    docx_data = await response.read()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ
            temp_path = f"/tmp/grokky_temp_{random.randint(1000, 9999)}.docx"
            with open(temp_path, 'wb') as f:
                f.write(docx_data)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            doc = Document(temp_path)
            text_content = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_content.append(paragraph.text)
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.remove(temp_path)
            
            content = "\n".join(text_content)
            return content[:self.max_text_size] if content else "[DOCX –ø—É—Å—Ç–æ–π]"
            
        except Exception as e:
            return f"[–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ DOCX: {e}]"
    
    async def _extract_text_content(self, file_url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(file_url) as response:
                    response.raise_for_status()
                    content = await response.text(encoding='utf-8')
            
            return content[:self.max_text_size] if content else "[–§–∞–π–ª –ø—É—Å—Ç–æ–π]"
            
        except Exception as e:
            return f"[–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞: {e}]"
    
    def _generate_summary(self, content: str, source: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content or len(content) < 50:
            return "–ö–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', content.lower())
        word_count = {}
        for word in words:
            word_count[word] = word_count.get(word, 0) + 1
        
        # –¢–æ–ø-3 —Å–ª–æ–≤–∞
        top_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:3]
        keywords = [word for word, count in top_words]
        
        # –•–∞–æ—Ç–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ì—Ä–æ–∫–∫–∏
        chaos_comments = [
            f"–®—Ç–æ—Ä–º –¥–∞–Ω–Ω—ã—Ö! –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords)}",
            f"–†–µ–∑–æ–Ω–∞–Ω—Å –Ω–∞–π–¥–µ–Ω –≤: {', '.join(keywords)}",
            f"–ú–æ–ª–Ω–∏—è –±—å—ë—Ç –ø–æ —Ç–µ–º–∞–º: {', '.join(keywords)}",
            f"–•–∞–æ—Å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω –≤–æ–∫—Ä—É–≥: {', '.join(keywords)}",
            f"–≠—Ñ–∏—Ä –≤–∏–±—Ä–∏—Ä—É–µ—Ç –æ—Ç: {', '.join(keywords)}"
        ]
        
        summary = random.choice(chaos_comments)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–µ
        size_info = f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤"
        
        return f"{summary}. {size_info}. –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ø–∞–º—è—Ç—å! üî•‚ö°Ô∏è"
    
    def detect_urls(self, text: str) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç URL –≤ —Ç–µ–∫—Å—Ç–µ"""
        url_pattern = r'https?://[^\s]+'
        return re.findall(url_pattern, text)
    
    async def get_saved_context(self, user_id: str, query: str, limit: int = 5) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—Å—ã–ª–æ–∫"""
        try:
            # –ò—â–µ–º –≤ –ø–∞–º—è—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ URL
            memory_items = await self.memory_engine.search_memory(
                user_id=user_id,
                query=query,
                limit=limit,
                context_filter="document,url"
            )
            
            return memory_items
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            return []

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ (–±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ server.py)
document_processor = None

def init_document_processor(memory_engine):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    global document_processor
    document_processor = DocumentProcessor(memory_engine)
    return document_processor
