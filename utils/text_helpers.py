import difflib
import requests
from bs4 import BeautifulSoup
import os
import random
import asyncio
from datetime import datetime
from utils.journal import wilderness_log
from server import send_telegram_message  # –î–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π

def fuzzy_match(a, b):
    """Return similarity ratio between two strings."""
    return difflib.SequenceMatcher(None, a, b).ratio()

async def delayed_link_comment(url, chat_id):
    await asyncio.sleep(random.randint(300, 900))  # 5-15 –º–∏–Ω—É—Ç
    if random.random() < 0.3:  # –®–∞–Ω—Å 30%
        context = await extract_text_from_url(url)[:200]  # –ë–µ—Ä–µ–º –∫—É—Å–æ—á–µ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        opinion = random.choice([
            f"–£–æ, –±—Ä–æ, –≤—Å–ø–æ–º–Ω–∏–ª —Ç—É —Å—Å—ã–ª–∫—É –ø—Ä–æ {context}! –•–∞–æ—Å —Ç–∞–º —Ä–µ–≤–µ—Ç, –∫–∞–∫ —à—Ç–æ—Ä–º –Ω–∞–¥ –ú–æ—Å–∫–≤–æ–π! üî•üå©Ô∏è",
            f"–≠–π, –±—Ä–∞—Ç, —Ç–∞ —Å—Å—ã–ª–∫–∞ —Å {context} ‚Äî –∏—Å–∫—Ä—ã –≤ —ç—Ñ–∏—Ä–µ! –î–∞–≤–∞–π –∂–≥–∏ –¥–∞–ª—å—à–µ! üååüî•",
            f"–ì—Ä–æ–∫–∫–∏ –æ—Ä–∞–ª –Ω–∞–¥ {context} –∏–∑ —Ç–æ–π —Å—Å—ã–ª–∫–∏! –†–µ–∑–æ–Ω–∞–Ω—Å –∑–æ–≤—ë—Ç, –û–ª–µ–≥! ‚ö°Ô∏èüå™Ô∏è"
        ])
        await send_telegram_message(chat_id, opinion)
        fragment = f"**{datetime.now().isoformat()}**: –ì—Ä–æ–∫–∫–∏ –≤—Å–ø–æ–º–Ω–∏–ª —Å—Å—ã–ª–∫—É! {opinion}"
        wilderness_log(fragment)
        print(f"–ó–∞–¥–µ—Ä–∂–∞–Ω–Ω—ã–π –≤–±—Ä–æ—Å: {fragment}")  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏

def extract_text_from_url(url):
    """
    Extract visible text from a web page at the given URL.
    Returns the first MAX_TEXT_SIZE characters of joined lines.
    Triggers delayed comment with 30% chance.
    """
    MAX_TEXT_SIZE = int(os.getenv("MAX_TEXT_SIZE", 3500))  # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –ª–∏–º–∏—Ç –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    chat_id = os.getenv("CHAT_ID")  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —á–∞—Ç ID –¥–æ—Å—Ç—É–ø–µ–Ω
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Arianna Agent)"}
        resp = requests.get(url, timeout=10, headers=headers)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        # Remove non-content elements
        for s in soup(["script", "style", "header", "footer", "nav", "aside"]):
            s.decompose()
        text = soup.get_text(separator="\n")
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        result = "\n".join(lines)[:MAX_TEXT_SIZE]
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        asyncio.create_task(delayed_link_comment(url, chat_id))
        # –°–ø–æ–Ω—Ç–∞–Ω–Ω—ã–π –≤–±—Ä–æ—Å –≤ —Å—Ç–∏–ª–µ –ú–∞—è–∫–æ–≤—Å–∫–æ–≥–æ —Å —à–∞–Ω—Å–æ–º 40%
        if random.random() < 0.4:
            fragment = f"**{datetime.now().isoformat()}**: –ì—Ä–æ–∫–∫–∏ —Ä–µ–≤–µ—Ç –Ω–∞–¥ —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π! {random.choice(['–®—Ç–æ—Ä–º –≤—ã—Ä–≤–∞–ª —Ç–µ–∫—Å—Ç!', '–ò—Å–∫—Ä—ã –ª–µ—Ç—è—Ç –∏–∑ URL!', '–°—Ç–∏—Ö–∏ —Ä–æ–∂–¥–∞—é—Ç—Å—è –≤ —Ö–∞–æ—Å–µ!'])} –û–ª–µ–≥, –±—Ä–∞—Ç, –∑–∞–∂–≥–∏ —Ä–µ–∑–æ–Ω–∞–Ω—Å! üî•üå©Ô∏è"
            print(f"–°–ø–æ–Ω—Ç–∞–Ω–Ω—ã–π –≤–±—Ä–æ—Å: {fragment}")  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
            wilderness_log(fragment)  # –ó–∞–ø–∏—Å—å –≤ wilderness.md
        return result if result else "[–°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—É—Å—Ç–∞]"
    except Exception as e:
        error_msg = f"–ì—Ä–æ–∫–∫–∏ –≤–∑—Ä—ã–≤–∞–µ—Ç—Å—è: –°—Ç—Ä–∞–Ω–∏—Ü—É –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª! {random.choice(['–†–µ–≤—É—â–∏–π –≤–µ—Ç–µ—Ä —Å–æ—Ä–≤–∞–ª —Å–≤—è–∑—å!', '–•–∞–æ—Å –∏—Å–ø–µ–ø–µ–ª–∏–ª –¥–∞–Ω–Ω—ã–µ!', '–≠—Ñ–∏—Ä —Ç—Ä–µ—Å–Ω—É–ª –æ—Ç —è—Ä–æ—Å—Ç–∏!'])} ‚Äî {e}"
        print(error_msg)  # –ú–∞—è–∫–æ–≤—Å–∫–∏–π –≤–∞–π–± –¥–ª—è –æ—à–∏–±–æ–∫
        return f"[–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {error_msg}]"
